<!DOCTYPE html>
<html lang="zh-TW">

<head>
	<meta charset="UTF-8">
	<link rel="stylesheet" href="main.css">
	<!-- <script src="main.js"></script> -->
	<title>MIRlab - Chih-Hsiang, Hsu RD Page</title>
</head>

<body>
	<div id="title">
		<h1 id="chih-hsiang-hsu">Chih-Hsiang, Hsu (許智翔)</h1>
		<hr>
	</div>
	<div id="toc_container">
		<p class="toc_title">Contents</p>
		<ul class="toc_list">
			<li><a href="#research">1 Research</a>
				<ul>
					<li><a href="#intro">1.1 Introduction</a></li>
				</ul>
				<ul>
					<li><a href="#paper-survey">1.2 Paper survey</a></li>
				</ul>
			</li>
			<li><a href="#meeting-slides">2 Meeting slides</a></li>
			<li><a href="#contact">3 Contact</a></li>
		</ul>
	</div>
	
	<div id="block">
		<h1 id="research">Research</h1>
		<hr>
		
		<h2 id="intro">Introduction</h2>
		We aims to do sports analysis by reconstructing 3D human pose from video source. This task can be split into
		two steps:
		<ul>
		<li>2D keypoints estimation: estimating the 2D keypoints on human body. The model we use is <a href="https://arxiv.org/abs/2104.02300">DEKR</a>.
		</li>
		<li>2D-to-3D lifting: reconstruct 3D human pose from a sequence of 2D keypoints. The model we use is
			<a href="https://arxiv.org/abs/2103.10455">Poseformer</a></li>
		</li>
		</ul>
		2D-to-3D lifting is actually an ill-posed problem. Many 3D poses can have the same 2D projection onto camera plane.
		For example, a person streching her arm forward has exactly the same 2D projection with her arm streching backward.
		We find that even the SOTA model has difficulty predicting the correct direction. This mostly happens on limbs.
		Hence we are finding a solution using pose hypotheses.<br>
		Given a seqeunce of predicted 3D human poses, there should be some erroneous pose with wrong depth. For each pose, 
		We generate several hypotheses of the same 2D projection and the answer should lie in these hypotheses. Then we use 
		dynamic programming considering the continuity of consequent poses to find a sequence of 3D human poses such that 
		it is closer to the answer.
		
		<h2 id="paper-survey">Paper survey</h2>
		<ul>
		<li>Human pose prior
			<ul>
			<li>ICCV 2021 - Probabilistic Monocular 3D Human Pose Estimation with Normalizing Flows
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2107.13788">https://arxiv.org/abs/2107.13788</a></li>
				<li>Code: <a href="https://github.com/twehrbein/Probabilistic-Monocular-3D-Human-Pose-Estimation-with-Normalizing-Flows">GitHub</a></li>
				</ul>
			</li>
			<li>CVPR 2023 - GFPose Learning 3D Human Pose Prior with Gradient Fields
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2212.08641">https://arxiv.org/abs/2212.08641</a></li>
				<li>Code: <a href="https://github.com/Embracing/GFPose">GitHub</a></li>
				<li>Notes: Using ground truth depth in normalization</li>
				</ul>
			</li>
			</ul>
		</li>

		<li>Absolute depth
			<ul>
			<li>3DV 2019 - Multi-Person 3D Human Pose Estimation from Monocular Images
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/1909.10854">https://arxiv.org/abs/1909.10854</a></li>
				<li>Notes: Estimating multi-person depth with focal length and translation optimization</li>
				</ul>
			</li>
			<li>ICCV 2019 - Camera Distance-aware Top-down Approach for 3D Multi-person Pose Estimation from a Single RGB Image
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/1907.11346">https://arxiv.org/abs/1907.11346</a></li>
				<li>Code: <a href="https://github.com/mks0601/3DMPPE_POSENET_RELEASE">GitHub</a></li>
				<li>Notes: Using bounding box in image and the 3d posture knowledge to estimate absolute depth</li>
				</ul>
			</li>
			<li>ICCV 2019 - Optimizing Network Structure for 3D Human Pose Estimation
				<ul>
				<li>Paper: <a href="https://ieeexplore.ieee.org/document/9010332">https://ieeexplore.ieee.org/document/9010332</a></li>
				<li>Code: <a href="https://github.com/CHUNYUWANG/lcn-pose">GitHub</a></li>
				<li>Notes: Using ground truth depth in normalization</li>
				</ul>
			</li>
			<li>ArXiv 2019 - PoseLifter: Absolute 3D human pose lifting network from a single noisy 2D human pose
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/1910.12029">https://arxiv.org/abs/1910.12029</a></li>
				<li>Code: <a href="https://github.com/juyongchang/PoseLifter">GitHub</a></li>
				<li>Notes: Predicting canonical root depth</li>
				</ul>
			</li>
			<li>ECCV 2020 - SMAP: Single-Shot Multi-Person Absolute 3D Pose Estimation
				<ul>
				<li>Paper: <a href="https://zju3dv.github.io/SMAP/">https://zju3dv.github.io/SMAP/</a></li>
				<li>Code: <a href="https://github.com/zju3dv/SMAP">GitHub</a></li>
				<li>Notes: Normalizing depth</li>
				</ul>
			</li>
			<li>CVPR 2022 - Ray3D: ray-based 3D human pose estimation for monocular absolute 3D localization
				<ul>
				<li>Paper: <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhan_Ray3D_Ray-Based_3D_Human_Pose_Estimation_for_Monocular_Absolute_3D_CVPR_2022_paper.pdf">https://openaccess.thecvf.com/content/CVPR2022/papers/Zhan_Ray3D_Ray-Based_3D_Human_Pose_Estimation_for_Monocular_Absolute_3D_CVPR_2022_paper.pdf</a></li>
				<li>Code: <a href="https://github.com/YxZhxn/Ray3D">GitHub</a></li>
				<li>Notes: Embedding 2d poses in projective space</li>
				</ul>
			</li>
			</ul>
		</li>

		<li>Occlusion handling
			<ul>
			<li>ICCV 2019 - Occlusion-Aware Networks for 3D Human Pose Estimation in Video
				<ul>
				<li>Paper: <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Cheng_Occlusion-Aware_Networks_for_3D_Human_Pose_Estimation_in_Video_ICCV_2019_paper.pdf">https://openaccess.thecvf.com/content_ICCV_2019/papers/Cheng_Occlusion-Aware_Networks_for_3D_Human_Pose_Estimation_in_Video_ICCV_2019_paper.pdf</a></li>
				<li>Notes: Detecting occlusion by cylinder-man model</li>
				</ul>
			</li>
			<li>AAAI 2020 - 3D Human Pose Estimation Using Spatio-Temporal Networks with Explicit Occlusion Training
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2004.11822">https://arxiv.org/abs/2004.11822</a></li>
				<li>Notes: Masking keypoints in training process</li>
				</ul>
			</li>
			<li>ECCV 2022 - Explicit Occlusion Reasoning for Multi-person 3D Human Pose Estimation
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2208.00090">https://arxiv.org/abs/2208.00090</a></li>
				<li>Code: <a href="https://github.com/qihao067/HUPOR">GitHub</a></li>
				<li>Notes: Detecting occlusion by Skeleton-guided human Shape Fitting</li>
				</ul>
			</li>
			</ul>
		</li>

		<li>2d keypoints estimation
			<ul>
			<li>CVPR 2019 - Deep High-Resolution Representation Learning for Human Pose Estimation
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/1902.09212">https://arxiv.org/abs/1902.09212</a></li>
				<li>Code: <a href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch">GitHub</a></li>
				</ul>
			</li>
			<li>CVPR 2021 - Bottom-Up Human Pose Estimation Via Disentangled Keypoint Regression
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2104.02300">https://arxiv.org/abs/2104.02300</a></li>
				<li>Code: <a href="https://github.com/HRNet/DEKR">GitHub</a></li>
				<li>Notes: best solution for 2d human pose estimation</li>
				</ul>
			</li>
			</ul>
		</li>

		<li>Multi-hypotheses pose estimation
			<ul>
			<li>ICCV 2017 - Generating Multiple Diverse Hypotheses for Human 3D Pose Consistent with 2D Joint Detections
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/1702.02258">https://arxiv.org/abs/1702.02258</a></li>
				</ul>
			</li>
			<li>CVPR 2019 - Generating Multiple Hypotheses for 3D Human Pose Estimation with Mixture Density Network
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/1904.05547">https://arxiv.org/abs/1904.05547</a></li>
				<li>Code: <a href="https://github.com/chaneyddtt/Generating-Multiple-Hypotheses-for-3D-Human-Pose-Estimation-with-Mixture-Density-Network">GitHub</a></li>
				</ul>
			</li>
			<li>CVPR 2022 - MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2111.12707">https://arxiv.org/abs/2111.12707</a></li>
				<li>Code: <a href="https://github.com/Vegetebird/MHFormer">GitHub</a></li>
				<li>Notes: there are several unsolved problems in this paper, which are mentioned in <a href="https://docs.google.com/presentation/d/1aXCfWwJpe3dKVOuIOHzL0LESQ9TrLYLG28u0oe9dU34/edit?usp=sharing">meeting slide 2022/11/02</a></li>
				</ul>
			</li>
			</ul>
		</li>

		<li>Geometry constraints on human pose estimation
			<ul>
			<li>CVPR 2019 - Self-Supervised Learning of 3D Human Pose using Multi-view Geometry
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/1903.02330">https://arxiv.org/abs/1903.02330</a></li>
				<li>Code: <a href="https://github.com/mkocabas/EpipolarPose">GitHub</a></li>
				<li>Notes: MPJPE (L2-norm loss) is said to be not sufficient</li>
				</ul>
			</li>
			<li>IEEE 2021 - Anatomy-aware 3D Human Pose Estimation with Bone-based Pose Decomposition
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2002.10322">https://arxiv.org/abs/2002.10322</a></li>
				<li>Code: <a href="https://github.com/sunnychencool/Anatomy3D">GitHub</a></li>
				</ul>
			</li>
			</ul>
		</li>

		<li>Human pose estimation
			<ul>
			<li>CVPR 2019 - 3D human pose estimation in video with temporal convolutions and semi-supervised training
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/1811.11742">https://arxiv.org/abs/1811.11742</a></li>
				<li>Code: <a href="https://github.com/facebookresearch/VideoPose3D">GitHub</a></li>
				</ul>
			</li>
			<li>CVPR 2019 - Semantic Graph Convolutional Networks for 3D Human Pose Regression
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/1904.03345">https://arxiv.org/abs/1904.03345</a></li>
				<li>Code: <a href="https://github.com/garyzhao/SemGCN">GitHub</a></li>
				</ul>
			</li>
			<li>2020 - A Graph Attention Spatio-temporal Convolutional Network for 3D Human Pose Estimation in Video
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2003.14179">https://arxiv.org/abs/2003.14179</a></li>
				<li>Code: <a href="https://github.com/fabro66/GAST-Net-3DPoseEstimation">GitHub</a></li>
				</ul>
			</li>
			<li>ICCV 2021 - 3D Human Pose Estimation with Spatial and Temporal Transformers
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2103.10455">https://arxiv.org/abs/2103.10455</a></li>
				<li>Code: <a href="https://github.com/zczcwh/PoseFormer">GitHub</a></li>
				</ul>
			</li>
			<li>IEEE 2022 - Limb Pose Aware Networks for Monocular 3D Pose Estimation
				<ul>
				<li>Paper: <a href="https://ieeexplore.ieee.org/document/9663053">https://ieeexplore.ieee.org/document/9663053</a></li>
				<li>Notes: Using additional information from raw images</li>
				</ul>
			</li>
			</ul>
		</li>

		<li>Transformer on computer vision
			<ul>
			<li>ICLR 2021 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a></li>
				<li>Code: <a href="https://github.com/google-research/vision_transformer">GitHub</a></li>
				</ul>
			</li>
			</ul>
		</li>

		<li>Object pose estimation in 6-dof
			<ul>
			<li>CVPR 2022 - EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2203.13254">https://arxiv.org/abs/2203.13254</a></li>
				<li>Code: <a href="https://github.com/tjiiv-cprg/EPro-PnP">GitHub</a></li>
				</ul>
			</li>
			</ul>
		</li>

		<li>IMU related
			<ul>
			<li>CVPR 2019 On the Continuity of Rotation Representations in Neural Networks
				<ul>
				<li>Paper: <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhou_On_the_Continuity_of_Rotation_Representations_in_Neural_Networks_CVPR_2019_paper.pdf">https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhou_On_the_Continuity_of_Rotation_Representations_in_Neural_Networks_CVPR_2019_paper.pdf</a></li>
				<li>Notes: representation of rotation</li>
				</ul>
			</li>
			<li>CVPR 2023 Physical Inertial Poser (PIP): Physics-aware Real-time Human Motion Tracking from Sparse Inertial Sensors
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2203.08528">https://arxiv.org/abs/2203.08528</a></li>
				<li>Code: <a href="https://github.com/Xinyu-Yi/PIP">GitHub</a></li>
				</ul>
			</li>
			<li>TMLR 2023 High fidelity neural audio compression
				<ul>
				<li>Paper: <a href="https://openreview.net/pdf?id=ivCd8z8zR2">https://openreview.net/pdf?id=ivCd8z8zR2</a></li>
				<li>Code: <a href="https://github.com/descriptinc/descript-audio-codec">GitHub</a></li>
				<li>Notes: imu data encoder</li>
				</ul>
			</li>
			</ul>
		</li>

		</ul>
	</div>
	<br/>
	
	<div id="block">
		<h1 id="meeting-slides">Meeting slides</h1>
		<hr>
		<ul>
			<li><a href="https://nas.mirlab.org/drive/d/s/sIRIwjlRnytY6ZYDaXx2693aA2xJLIYN/Sy7WV-aNePAMw0FKVVtvZDbmM3CwLJu0-7Ldg8LZnTgo">Introduction</a> - Introduction of our research.</li>
		</ul>
		<ul>
			<li><a href="https://docs.google.com/presentation/d/1aXCfWwJpe3dKVOuIOHzL0LESQ9TrLYLG28u0oe9dU34/edit?usp=sharing">2022/11/02</a> - Introducing recent models on multi-hypotheses 3d pose estimating. Discussing over possible improvement.</li>
		</ul>
		<ul>
			<li><a href="https://nas.mirlab.org/drive/d/s/rv8tBSq5beeDah0qo9wgkdMNY1qlFrbE/ny7TMAnPksAJ0uwVbuqeBOO7-g97BtNy-e7gglihoTgo">2023/01/11</a> - Estimating state probability with neural network.</li>
		</ul>
		<ul>
			<li><a href="https://nas.mirlab.org/drive/d/s/sOBn51kkZTxt1sKahNmC7PAaquWvjCfE/_SPcEm10pI19OUtJuo1cHG2YwztqVr6I-8LiAla5oTgo">2023/02/15</a> - Introducing dynamic programming with GMM-based probability estimation.</li>
		</ul>
		<ul>
			<li><a href="https://nas.mirlab.org/drive/d/s/srFtw3AW1gknqaAyusmIHor9AaeKHGpb/7fserjpTfm890R65NL3gbqFW8wKabQxt-grngXvxoTgo">2023/03/22</a> - Introducing tri-skeleton in the dynamic programming.</li>
		</ul>
		<ul>
			<li><a href="https://nas.mirlab.org/drive/d/s/sx2b5UUEZTFdmql7Mg6aqXFC2ypcbe8o/8Qn18p4BaAjnDVP5aMwRkzVgY9Cifr5t-qLXg6sF8ego">2023/03/29</a> - Result of using tri-skeleton in the dynamic programming.</li>
		</ul>
		<ul>
			<li><a href="https://nas.mirlab.org/drive/d/s/thWNXkGcmDiNT5XTNYZfsSUNRCCD5I0x/pJcAcwfLmxWeUl735DEGu9cU_TkofB3V-Yb3gGgvzdgo">2023/05/24</a> - Exploiting information from 2d video, e.g., intrinsic parameters and absolute depth</li>
		</ul>
	</div>
	
	<br/>

	<div id="block">
		<h1 id="contact">Contact</h1>
		<hr>
		<ul>
			<li>Email: andy.hsu@nas.mirlab.org</li>
			<li>Lab: <a href="http://mirlab.org/">Multimedia Information Retrieval Lab</a></li>
		</ul>
	</div>
</body>

</html>
