<!DOCTYPE html>
<html lang="zh-TW">

<head>
<meta name="google-site-verification" content="cqb2YUCPgLkHv6XuA-4T37xn4zO5CHOCbVcTkZkDLNE" />
	<meta charset="UTF-8">
	<link rel="stylesheet" href="main.css">
	<!-- <script src="main.js"></script> -->
	<title>MIRlab - Chih-Hsiang, Hsu RD Page</title>
</head>

<body>
	<div id="title">
		<h1 id="chih-hsiang-hsu">Chih-Hsiang, Hsu (許智翔)</h1>
		<hr>
	</div>
	<div id="toc_container">
		<p class="toc_title">Contents</p>
		<ul class="toc_list">
			<li><a href="#research">1 Research</a>
				<ul>
					<li><a href="#intro">1.1 Introduction</a></li>
				</ul>
				<ul>
					<li><a href="#paper-survey">1.2 Paper survey</a></li>
				</ul>
			</li>
			<li><a href="#meeting-slides">2 Meeting slides</a></li>
			<li><a href="#contact">3 Contact</a></li>
		</ul>
	</div>
	
	<div id="block">
		<h1 id="research">Research</h1>
		<hr>
		
		<h2 id="intro">Introduction</h2>
		We aims to do sports analysis by reconstructing 3D human pose from video source. This task can be split into
		two steps:
		<ul>
		<li>2D keypoints estimation: estimating the 2D keypoints on human body, e.g. <a href="https://arxiv.org/abs/2104.02300">DEKR</a>.
		</li>
		<li>2D-to-3D lifting: reconstruct 3D human pose from a sequence of 2D keypoints, e.g. <a href="https://arxiv.org/abs/2103.10455">Poseformer</a></li>
		</li>
		</ul>
		
		<h2 id="paper-survey">Paper survey</h2>
		<ul>
		<li>Human pose prior
			<ul>
			<li>ICCV 2021 - Probabilistic Monocular 3D Human Pose Estimation with Normalizing Flows
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2107.13788">https://arxiv.org/abs/2107.13788</a></li>
				<li>Code: <a href="https://github.com/twehrbein/Probabilistic-Monocular-3D-Human-Pose-Estimation-with-Normalizing-Flows">GitHub</a></li>
				</ul>
			</li>
			<li>CVPR 2023 - GFPose Learning 3D Human Pose Prior with Gradient Fields
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2212.08641">https://arxiv.org/abs/2212.08641</a></li>
				<li>Code: <a href="https://github.com/Embracing/GFPose">GitHub</a></li>
				<li>Notes: Using ground truth depth in normalization</li>
				</ul>
			</li>
			</ul>
		</li>

		<li>Absolute depth
			<ul>
			<li>3DV 2019 - Multi-Person 3D Human Pose Estimation from Monocular Images
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/1909.10854">https://arxiv.org/abs/1909.10854</a></li>
				<li>Notes: Estimating multi-person depth with focal length and translation optimization</li>
				</ul>
			</li>
			<li>ICCV 2019 - Camera Distance-aware Top-down Approach for 3D Multi-person Pose Estimation from a Single RGB Image
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/1907.11346">https://arxiv.org/abs/1907.11346</a></li>
				<li>Code: <a href="https://github.com/mks0601/3DMPPE_POSENET_RELEASE">GitHub</a></li>
				<li>Notes: Using bounding box in image and the 3d posture knowledge to estimate absolute depth</li>
				</ul>
			</li>
			<li>ICCV 2019 - Optimizing Network Structure for 3D Human Pose Estimation
				<ul>
				<li>Paper: <a href="https://ieeexplore.ieee.org/document/9010332">https://ieeexplore.ieee.org/document/9010332</a></li>
				<li>Code: <a href="https://github.com/CHUNYUWANG/lcn-pose">GitHub</a></li>
				<li>Notes: Using ground truth depth in normalization</li>
				</ul>
			</li>
			<li>ArXiv 2019 - PoseLifter: Absolute 3D human pose lifting network from a single noisy 2D human pose
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/1910.12029">https://arxiv.org/abs/1910.12029</a></li>
				<li>Code: <a href="https://github.com/juyongchang/PoseLifter">GitHub</a></li>
				<li>Notes: Predicting canonical root depth</li>
				</ul>
			</li>
			<li>ECCV 2020 - SMAP: Single-Shot Multi-Person Absolute 3D Pose Estimation
				<ul>
				<li>Paper: <a href="https://zju3dv.github.io/SMAP/">https://zju3dv.github.io/SMAP/</a></li>
				<li>Code: <a href="https://github.com/zju3dv/SMAP">GitHub</a></li>
				<li>Notes: Normalizing depth</li>
				</ul>
			</li>
			<li>CVPR 2022 - Ray3D: ray-based 3D human pose estimation for monocular absolute 3D localization
				<ul>
				<li>Paper: <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhan_Ray3D_Ray-Based_3D_Human_Pose_Estimation_for_Monocular_Absolute_3D_CVPR_2022_paper.pdf">https://openaccess.thecvf.com/content/CVPR2022/papers/Zhan_Ray3D_Ray-Based_3D_Human_Pose_Estimation_for_Monocular_Absolute_3D_CVPR_2022_paper.pdf</a></li>
				<li>Code: <a href="https://github.com/YxZhxn/Ray3D">GitHub</a></li>
				<li>Notes: Embedding 2d poses in projective space</li>
				</ul>
			</li>
			</ul>
		</li>

		<li>Occlusion handling
			<ul>
			<li>ICCV 2019 - Occlusion-Aware Networks for 3D Human Pose Estimation in Video
				<ul>
				<li>Paper: <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Cheng_Occlusion-Aware_Networks_for_3D_Human_Pose_Estimation_in_Video_ICCV_2019_paper.pdf">https://openaccess.thecvf.com/content_ICCV_2019/papers/Cheng_Occlusion-Aware_Networks_for_3D_Human_Pose_Estimation_in_Video_ICCV_2019_paper.pdf</a></li>
				<li>Notes: Detecting occlusion by cylinder-man model</li>
				</ul>
			</li>
			<li>AAAI 2020 - 3D Human Pose Estimation Using Spatio-Temporal Networks with Explicit Occlusion Training
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2004.11822">https://arxiv.org/abs/2004.11822</a></li>
				<li>Notes: Masking keypoints in training process</li>
				</ul>
			</li>
			<li>ECCV 2022 - Explicit Occlusion Reasoning for Multi-person 3D Human Pose Estimation
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2208.00090">https://arxiv.org/abs/2208.00090</a></li>
				<li>Code: <a href="https://github.com/qihao067/HUPOR">GitHub</a></li>
				<li>Notes: Detecting occlusion by Skeleton-guided human Shape Fitting</li>
				</ul>
			</li>
			</ul>
		</li>

		<li>2d keypoints estimation
			<ul>
			<li>CVPR 2019 - Deep High-Resolution Representation Learning for Human Pose Estimation
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/1902.09212">https://arxiv.org/abs/1902.09212</a></li>
				<li>Code: <a href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch">GitHub</a></li>
				</ul>
			</li>
			<li>CVPR 2021 - Bottom-Up Human Pose Estimation Via Disentangled Keypoint Regression
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2104.02300">https://arxiv.org/abs/2104.02300</a></li>
				<li>Code: <a href="https://github.com/HRNet/DEKR">GitHub</a></li>
				<li>Notes: best solution for 2d human pose estimation</li>
				</ul>
			</li>
			</ul>
		</li>

		<li>Multi-hypotheses pose estimation
			<ul>
			<li>ICCV 2017 - Generating Multiple Diverse Hypotheses for Human 3D Pose Consistent with 2D Joint Detections
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/1702.02258">https://arxiv.org/abs/1702.02258</a></li>
				</ul>
			</li>
			<li>CVPR 2019 - Generating Multiple Hypotheses for 3D Human Pose Estimation with Mixture Density Network
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/1904.05547">https://arxiv.org/abs/1904.05547</a></li>
				<li>Code: <a href="https://github.com/chaneyddtt/Generating-Multiple-Hypotheses-for-3D-Human-Pose-Estimation-with-Mixture-Density-Network">GitHub</a></li>
				</ul>
			</li>
			<li>CVPR 2022 - MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2111.12707">https://arxiv.org/abs/2111.12707</a></li>
				<li>Code: <a href="https://github.com/Vegetebird/MHFormer">GitHub</a></li>
				<li>Notes: there are several unsolved problems in this paper, which are mentioned in <a href="https://docs.google.com/presentation/d/1aXCfWwJpe3dKVOuIOHzL0LESQ9TrLYLG28u0oe9dU34/edit?usp=sharing">meeting slide 2022/11/02</a></li>
				</ul>
			</li>
			<li>CVPR 2023 -  Diffpose: Toward more reliable 3d pose estimation
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2211.16940">https://arxiv.org/abs/2211.16940</a></li>
				<li>Code: <a href="https://github.com/GONGJIA0208/Diffpose">GitHub</a></li>
				</ul>
			</li>
			<li>ICCV 2023 - Diffusion-based 3d human pose estimation with multi-hypothesis aggregation
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2303.11579">https://arxiv.org/abs/2303.11579</a></li>
				<li>Code: <a href="https://github.com/paTRICK-swk/D3DP">GitHub</a></li>
				</ul>
			</li>
			<li>CVPR 2024 - Ktpformer: Kinematics and trajectory prior knowledge-enhanced transformer for 3d human pose estimation
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2404.00658">https://arxiv.org/abs/2404.00658</a></li>
				<li>Code: <a href="https://github.com/JihuaPeng/KTPFormer">GitHub</a></li>
				</ul>
			</li>
			<li>CVPR 2024 - Finepose: Fine-grained prompt-driven 3d human pose estimation via diffusion models
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2405.05216">https://arxiv.org/abs/2405.05216</a></li>
				<li>Code: <a href="https://github.com/PKU-ICST-MIPL/FinePOSE_CVPR2024">GitHub</a></li>
				</ul>
			</li>
			</ul>
		</li>

		<li>Geometry constraints on human pose estimation
			<ul>
			<li>CVPR 2019 - Self-Supervised Learning of 3D Human Pose using Multi-view Geometry
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/1903.02330">https://arxiv.org/abs/1903.02330</a></li>
				<li>Code: <a href="https://github.com/mkocabas/EpipolarPose">GitHub</a></li>
				<li>Notes: MPJPE (L2-norm loss) is said to be not sufficient</li>
				</ul>
			</li>
			<li>IEEE 2021 - Anatomy-aware 3D Human Pose Estimation with Bone-based Pose Decomposition
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2002.10322">https://arxiv.org/abs/2002.10322</a></li>
				<li>Code: <a href="https://github.com/sunnychencool/Anatomy3D">GitHub</a></li>
				</ul>
			</li>
			</ul>
		</li>

		<li>Human pose estimation
			<ul>
			<li>CVPR 2019 - 3D human pose estimation in video with temporal convolutions and semi-supervised training
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/1811.11742">https://arxiv.org/abs/1811.11742</a></li>
				<li>Code: <a href="https://github.com/facebookresearch/VideoPose3D">GitHub</a></li>
				</ul>
			</li>
			<li>CVPR 2019 - Semantic Graph Convolutional Networks for 3D Human Pose Regression
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/1904.03345">https://arxiv.org/abs/1904.03345</a></li>
				<li>Code: <a href="https://github.com/garyzhao/SemGCN">GitHub</a></li>
				</ul>
			</li>
			<li>2020 - A Graph Attention Spatio-temporal Convolutional Network for 3D Human Pose Estimation in Video
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2003.14179">https://arxiv.org/abs/2003.14179</a></li>
				<li>Code: <a href="https://github.com/fabro66/GAST-Net-3DPoseEstimation">GitHub</a></li>
				</ul>
			</li>
			<li>ICCV 2021 - 3D Human Pose Estimation with Spatial and Temporal Transformers
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2103.10455">https://arxiv.org/abs/2103.10455</a></li>
				<li>Code: <a href="https://github.com/zczcwh/PoseFormer">GitHub</a></li>
				</ul>
			</li>
			<li>IEEE 2022 - Limb Pose Aware Networks for Monocular 3D Pose Estimation
				<ul>
				<li>Paper: <a href="https://ieeexplore.ieee.org/document/9663053">https://ieeexplore.ieee.org/document/9663053</a></li>
				<li>Notes: Using additional information from raw images</li>
				</ul>
			</li>
			<li>CVPR 2022 - Mixste: Seq2seq mixed spatiotemporal encoder for 3d human pose estimation in video
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2203.00859">https://arxiv.org/abs/2203.00859</a></li>
				<li>Code: <a href="https://github.com/JinluZhang1126/MixSTE">GitHub</a></li>
				</ul>
			</li>
			<li>CVPR 2023 - 3d human pose estimation with spatio-temporal criss-cross attention
				<ul>
				<li>Paper: <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_3D_Human_Pose_Estimation_With_Spatio-Temporal_Criss-Cross_Attention_CVPR_2023_paper.pdf">https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_3D_Human_Pose_Estimation_With_Spatio-Temporal_Criss-Cross_Attention_CVPR_2023_paper.pdf</a></li>
				<li>Code: <a href="https://github.com/zhenhuat/STCFormer">GitHub</a></li>
				</ul>
			</li>
			<li>CVPR 2023 -  Poseformerv2: Exploring frequency domain for efficient and robust 3d human pose estimation
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2303.17472">https://arxiv.org/abs/2303.17472</a></li>
				<li>Code: <a href="https://github.com/QitaoZhao/PoseFormerV2">GitHub</a></li>
				</ul>
			</li>
			</ul>
		</li>

		<li>Transformer on computer vision
			<ul>
			<li>ICLR 2021 - An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a></li>
				<li>Code: <a href="https://github.com/google-research/vision_transformer">GitHub</a></li>
				</ul>
			</li>
			</ul>
		</li>

		<li>Object pose estimation in 6-dof
			<ul>
			<li>CVPR 2022 - EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2203.13254">https://arxiv.org/abs/2203.13254</a></li>
				<li>Code: <a href="https://github.com/tjiiv-cprg/EPro-PnP">GitHub</a></li>
				</ul>
			</li>
			</ul>
		</li>

		<li>IMU related
			<ul>
			<li>CVPR 2019 - On the Continuity of Rotation Representations in Neural Networks
				<ul>
				<li>Paper: <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhou_On_the_Continuity_of_Rotation_Representations_in_Neural_Networks_CVPR_2019_paper.pdf">https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhou_On_the_Continuity_of_Rotation_Representations_in_Neural_Networks_CVPR_2019_paper.pdf</a></li>
				<li>Notes: representation of rotation</li>
				</ul>
			</li>
			<li>CVPR 2023 - Physical Inertial Poser (PIP): Physics-aware Real-time Human Motion Tracking from Sparse Inertial Sensors
				<ul>
				<li>Paper: <a href="https://arxiv.org/abs/2203.08528">https://arxiv.org/abs/2203.08528</a></li>
				<li>Code: <a href="https://github.com/Xinyu-Yi/PIP">GitHub</a></li>
				</ul>
			</li>
			<li>TMLR 2023 - High fidelity neural audio compression
				<ul>
				<li>Paper: <a href="https://openreview.net/pdf?id=ivCd8z8zR2">https://openreview.net/pdf?id=ivCd8z8zR2</a></li>
				<li>Code: <a href="https://github.com/descriptinc/descript-audio-codec">GitHub</a></li>
				<li>Notes: imu data encoder</li>
				</ul>
			</li>
			</ul>
		</li>

		</ul>

	</div>
	<br/>
	
	<div id="block">
		<h1 id="contact">Contact</h1>
		<hr>
		<ul>
			<li>Email: andy.hsu@nas.mirlab.org</li>
			<li>Lab: <a href="http://mirlab.org/">Multimedia Information Retrieval Lab</a></li>
		</ul>
	</div>
</body>

</html>
